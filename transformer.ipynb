{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPExXx6GNXDJG0GZIWyM/HH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakib103/pancancer_classification/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvL_k-vapdJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpd6-IrppjS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "19843141705b44d69f89bc4c63a32099",
            "6867c453776646fcae1abbccfd6fb9ad",
            "eba8107e3e0a4b62a5aed34255bd3df7",
            "b2a71ccae9dd4ad7a57232fdbecacfcc",
            "10ffd5953e4b4d2385ac0c8f472c0c1b",
            "68e312563e0d44d8b6157c17b1c9155d",
            "9c2607f36002406281ec9549c548e5e0",
            "4ae155345e7a447aa89c500188c794cf",
            "6bd1c4ee45c8475fbe33ae047fe3512a",
            "06525fd5a21440e28c068785197b1930",
            "eccbaffa9e454bad975fb5018d5257c1",
            "19ddbf506b33417d9ff7b27b5c7262dc",
            "af6068f045e24b5f8c52826451d4336f",
            "624912d7093749dca6cf2c9352fcee02",
            "09992cbc7abc450aac9d1fbc517362ae",
            "791c3f74d6694f2dad692f3d904ead71",
            "8420e60ffdde4328839c8d077d660866",
            "5b06cb2d5ce54e53948363da37092ebc",
            "c70e147330694947b0d57cdacf8812cd",
            "368e740bdb144c39971bc9501da8468d",
            "a6bb4e3baeaf47aea0f8c8df90bfba07",
            "4c94ca52781c4e83a425e9f40bb34910",
            "35a6b912c2b846f9a2d28cfca00bbf80",
            "dc5cbc1ff75f428cb6e5d4793f1badc1",
            "325d69f9b97642ad8f24ce7cf1a30a5d",
            "cc69c6fa4d214c08829e5f1513aa19cf",
            "aaa9f2308bdf426dbdb3ec92b5dba025",
            "bddfb9d21baf42b28b526a12d3532cb1",
            "9ad9708998d24da19f1b277ab12062d2",
            "4f6540788ecf4b2d9c39cd09bed33379",
            "787c1a2ac1fa43838f2e34fbbee2f6b1",
            "f209fa05f8c44644b6df246cfff2e95a",
            "3020ccd42c8147c1a16b425e6ea9ac48",
            "4c7fc6ba084a44be942d84c5e5ea0a94",
            "cf72c370de8843899c549bc85f812ef5",
            "b0314f5a243049938f81ba40fc97ca8a",
            "a71d55a7e9644080ac794704e84b0afc",
            "c74725381c024d8ea346d05d6e736bdd",
            "62e3511d35f54a039d1ea55d7bd47ce4",
            "ecc6e5d00b854808b87fe2c15179ffa1",
            "d5c44dad6fdc4899837ff6bb9906dea5",
            "afb4614c437342ca98ad4bc9543b3af7",
            "4854294742f34905b04ed9c32d73d231",
            "a1700a86b67943f5990137bd2172721a",
            "7ccf0458e8cc40848e44f6ca4a0741da",
            "d4014c119aae4447b0db6f7f10cac708",
            "ca29c1a056ba45a1bb29ac11db640b2e",
            "710685375e2441729befcabbfcc6b965",
            "3a2a3cf35672461c985d31a97e658ad2",
            "3b99a8d542344fa385ab2a7cf7e26f1f",
            "903ce5ee6f364e1595b23c75ddb5eb5d",
            "f406a1b8de5c40c2bcb053e329a54e20",
            "fdcd5f1d371f48ca990cd6bb0e8bdc09",
            "0e5d87e8446842a7828cb96e8545ea30",
            "281d3a62e783426f8f8f5cdc988e2869",
            "02f6f85d9eb343809a0c505e52833123",
            "e2d7ea066c8c475c9d9e874199374e55",
            "3ab2e4a097d449c89e8b68e25825022a",
            "5b7dd02920924fcd87130ac7767c9fff",
            "cae6fd892e6f43e6a0e6473b85cd90df",
            "bcf8a143765043d6bb213a55ea67f9c3",
            "a8d1f122c25d491584edf33748fd98f6",
            "8b7eb08c813a401e854b0cdbf8f67906",
            "616446566b304bf88d0df4ad9e42bc25",
            "d18c4226b1ba43f3a498ee047da1409b",
            "e1ba38bad55e4f118c579aba2b54aad0",
            "5f359d8e82014dbf82c65e6064107a82",
            "d68693a1712b4786830604b28a2c6341",
            "bf4123804bdb49a68c96e62ed16d99d1",
            "63c7e0e129e846cd9ca909c673e56934",
            "c432ee57af42418cab6b4dc53fe68c4f",
            "936f60a5bce045bb8e032494172d6a2e"
          ]
        },
        "outputId": "7e06085f-4a19-469f-bb32-e333a9afeb2f"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19843141705b44d69f89bc4c63a32099",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bd1c4ee45c8475fbe33ae047fe3512a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8420e60ffdde4328839c8d077d660866",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "325d69f9b97642ad8f24ce7cf1a30a5d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incomplete7VZH8J/ted_hrlr_translate-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3020ccd42c8147c1a16b425e6ea9ac48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=51785.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5c44dad6fdc4899837ff6bb9906dea5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incomplete7VZH8J/ted_hrlr_translate-validation.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a2a3cf35672461c985d31a97e658ad2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1193.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d7ea066c8c475c9d9e874199374e55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incomplete7VZH8J/ted_hrlr_translate-test.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d18c4226b1ba43f3a498ee047da1409b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1803.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UWPLCJsprEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TVSMqPSpvTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "16417f35-a804-4b89-c938-ea7c8ed4e3a5"
      },
      "source": [
        "sample_string = 'Transformer is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
            "The original string: Transformer is awesome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys5QX9m9p-NJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "82244116-f1a9-4bc7-b02f-fe65633d9454"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7915 ----> T\n",
            "1248 ----> ran\n",
            "7946 ----> s\n",
            "7194 ----> former \n",
            "13 ----> is \n",
            "2799 ----> awesome\n",
            "7877 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ4DWJrAqAxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PeqofyMqfhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2\n",
        "\n",
        "def tf_encode(pt, en):\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en\n",
        "\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)\n",
        "  \n",
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJ1Nef2gokp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03b-X91PgxJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TTxtM-qK8uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTAggO2ZMGEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0f54fccc-0d99-4ab6-a07a-fa097e402622"
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZkFTSMhH7aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAapuyQ_d_px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwTPmHyQeUSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sQAEPxkeeOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B_kIa36enCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiH9P7shexSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKjAYFLsekBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP5daWpDjZHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 64 #128\n",
        "dff = 64 #512\n",
        "num_heads = 4 #8\n",
        "\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaUV8hJUe9W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0-FSzXFfAr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSSz7FvYfI1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3hAbYyufiT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVX7Rl0rfhl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7GaH2qefqny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szSRjM5Zft-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_BmkZ5DfxRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7xme5osf0JG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZZ0YnZef0p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHVRpPVlf3L0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8049f240-5f9a-4f72-86ad-720d95c56b02"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.9953 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.9723 Accuracy 0.0001\n",
            "Epoch 1 Batch 100 Loss 8.9234 Accuracy 0.0035\n",
            "Epoch 1 Batch 150 Loss 8.8574 Accuracy 0.0096\n",
            "Epoch 1 Batch 200 Loss 8.7703 Accuracy 0.0125\n",
            "Epoch 1 Batch 250 Loss 8.6591 Accuracy 0.0143\n",
            "Epoch 1 Batch 300 Loss 8.5259 Accuracy 0.0159\n",
            "Epoch 1 Batch 350 Loss 8.3758 Accuracy 0.0175\n",
            "Epoch 1 Batch 400 Loss 8.2188 Accuracy 0.0187\n",
            "Epoch 1 Batch 450 Loss 8.0605 Accuracy 0.0197\n",
            "Epoch 1 Batch 500 Loss 7.9131 Accuracy 0.0210\n",
            "Epoch 1 Batch 550 Loss 7.7800 Accuracy 0.0235\n",
            "Epoch 1 Batch 600 Loss 7.6640 Accuracy 0.0258\n",
            "Epoch 1 Batch 650 Loss 7.5544 Accuracy 0.0282\n",
            "Epoch 1 Batch 700 Loss 7.4501 Accuracy 0.0309\n",
            "Epoch 1 Loss 7.4459 Accuracy 0.0310\n",
            "Time taken for 1 epoch: 512.0592725276947 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.8824 Accuracy 0.0707\n",
            "Epoch 2 Batch 50 Loss 5.9075 Accuracy 0.0768\n",
            "Epoch 2 Batch 100 Loss 5.8337 Accuracy 0.0820\n",
            "Epoch 2 Batch 150 Loss 5.7665 Accuracy 0.0854\n",
            "Epoch 2 Batch 200 Loss 5.7045 Accuracy 0.0884\n",
            "Epoch 2 Batch 250 Loss 5.6487 Accuracy 0.0912\n",
            "Epoch 2 Batch 300 Loss 5.5974 Accuracy 0.0939\n",
            "Epoch 2 Batch 350 Loss 5.5448 Accuracy 0.0963\n",
            "Epoch 2 Batch 400 Loss 5.5049 Accuracy 0.0982\n",
            "Epoch 2 Batch 450 Loss 5.4659 Accuracy 0.1003\n",
            "Epoch 2 Batch 500 Loss 5.4256 Accuracy 0.1021\n",
            "Epoch 2 Batch 550 Loss 5.3884 Accuracy 0.1039\n",
            "Epoch 2 Batch 600 Loss 5.3527 Accuracy 0.1055\n",
            "Epoch 2 Batch 650 Loss 5.3206 Accuracy 0.1070\n",
            "Epoch 2 Batch 700 Loss 5.2926 Accuracy 0.1085\n",
            "Epoch 2 Loss 5.2913 Accuracy 0.1085\n",
            "Time taken for 1 epoch: 463.3611259460449 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.7097 Accuracy 0.1361\n",
            "Epoch 3 Batch 50 Loss 4.8232 Accuracy 0.1309\n",
            "Epoch 3 Batch 100 Loss 4.8073 Accuracy 0.1309\n",
            "Epoch 3 Batch 150 Loss 4.7968 Accuracy 0.1315\n",
            "Epoch 3 Batch 200 Loss 4.7883 Accuracy 0.1318\n",
            "Epoch 3 Batch 250 Loss 4.7796 Accuracy 0.1319\n",
            "Epoch 3 Batch 300 Loss 4.7678 Accuracy 0.1322\n",
            "Epoch 3 Batch 350 Loss 4.7572 Accuracy 0.1331\n",
            "Epoch 3 Batch 400 Loss 4.7449 Accuracy 0.1338\n",
            "Epoch 3 Batch 450 Loss 4.7306 Accuracy 0.1347\n",
            "Epoch 3 Batch 500 Loss 4.7181 Accuracy 0.1353\n",
            "Epoch 3 Batch 550 Loss 4.7114 Accuracy 0.1357\n",
            "Epoch 3 Batch 600 Loss 4.6983 Accuracy 0.1365\n",
            "Epoch 3 Batch 650 Loss 4.6869 Accuracy 0.1369\n",
            "Epoch 3 Batch 700 Loss 4.6738 Accuracy 0.1376\n",
            "Epoch 3 Loss 4.6737 Accuracy 0.1376\n",
            "Time taken for 1 epoch: 463.64747381210327 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.5048 Accuracy 0.1435\n",
            "Epoch 4 Batch 50 Loss 4.4565 Accuracy 0.1446\n",
            "Epoch 4 Batch 100 Loss 4.4240 Accuracy 0.1463\n",
            "Epoch 4 Batch 150 Loss 4.4228 Accuracy 0.1478\n",
            "Epoch 4 Batch 200 Loss 4.4144 Accuracy 0.1487\n",
            "Epoch 4 Batch 250 Loss 4.4139 Accuracy 0.1491\n",
            "Epoch 4 Batch 300 Loss 4.4045 Accuracy 0.1497\n",
            "Epoch 4 Batch 350 Loss 4.4006 Accuracy 0.1498\n",
            "Epoch 4 Batch 400 Loss 4.3941 Accuracy 0.1502\n",
            "Epoch 4 Batch 450 Loss 4.3862 Accuracy 0.1506\n",
            "Epoch 4 Batch 500 Loss 4.3801 Accuracy 0.1509\n",
            "Epoch 4 Batch 550 Loss 4.3727 Accuracy 0.1516\n",
            "Epoch 4 Batch 600 Loss 4.3652 Accuracy 0.1521\n",
            "Epoch 4 Batch 650 Loss 4.3551 Accuracy 0.1527\n",
            "Epoch 4 Batch 700 Loss 4.3481 Accuracy 0.1531\n",
            "Epoch 4 Loss 4.3480 Accuracy 0.1531\n",
            "Time taken for 1 epoch: 467.2472767829895 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.1744 Accuracy 0.1623\n",
            "Epoch 5 Batch 50 Loss 4.1151 Accuracy 0.1650\n",
            "Epoch 5 Batch 100 Loss 4.1096 Accuracy 0.1634\n",
            "Epoch 5 Batch 150 Loss 4.0911 Accuracy 0.1649\n",
            "Epoch 5 Batch 200 Loss 4.0899 Accuracy 0.1649\n",
            "Epoch 5 Batch 250 Loss 4.0811 Accuracy 0.1659\n",
            "Epoch 5 Batch 300 Loss 4.0708 Accuracy 0.1669\n",
            "Epoch 5 Batch 350 Loss 4.0575 Accuracy 0.1674\n",
            "Epoch 5 Batch 400 Loss 4.0465 Accuracy 0.1681\n",
            "Epoch 5 Batch 450 Loss 4.0351 Accuracy 0.1690\n",
            "Epoch 5 Batch 500 Loss 4.0247 Accuracy 0.1701\n",
            "Epoch 5 Batch 550 Loss 4.0137 Accuracy 0.1709\n",
            "Epoch 5 Batch 600 Loss 4.0045 Accuracy 0.1717\n",
            "Epoch 5 Batch 650 Loss 3.9944 Accuracy 0.1725\n",
            "Epoch 5 Batch 700 Loss 3.9834 Accuracy 0.1732\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.9831 Accuracy 0.1732\n",
            "Time taken for 1 epoch: 463.4683926105499 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.6926 Accuracy 0.1735\n",
            "Epoch 6 Batch 50 Loss 3.6706 Accuracy 0.1896\n",
            "Epoch 6 Batch 100 Loss 3.6615 Accuracy 0.1910\n",
            "Epoch 6 Batch 150 Loss 3.6579 Accuracy 0.1911\n",
            "Epoch 6 Batch 200 Loss 3.6560 Accuracy 0.1917\n",
            "Epoch 6 Batch 250 Loss 3.6529 Accuracy 0.1920\n",
            "Epoch 6 Batch 300 Loss 3.6453 Accuracy 0.1922\n",
            "Epoch 6 Batch 350 Loss 3.6390 Accuracy 0.1928\n",
            "Epoch 6 Batch 400 Loss 3.6314 Accuracy 0.1939\n",
            "Epoch 6 Batch 450 Loss 3.6196 Accuracy 0.1944\n",
            "Epoch 6 Batch 500 Loss 3.6121 Accuracy 0.1949\n",
            "Epoch 6 Batch 550 Loss 3.6021 Accuracy 0.1957\n",
            "Epoch 6 Batch 600 Loss 3.5956 Accuracy 0.1962\n",
            "Epoch 6 Batch 650 Loss 3.5871 Accuracy 0.1968\n",
            "Epoch 6 Batch 700 Loss 3.5790 Accuracy 0.1972\n",
            "Epoch 6 Loss 3.5784 Accuracy 0.1973\n",
            "Time taken for 1 epoch: 465.5450143814087 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.1714 Accuracy 0.2043\n",
            "Epoch 7 Batch 50 Loss 3.2670 Accuracy 0.2144\n",
            "Epoch 7 Batch 100 Loss 3.2747 Accuracy 0.2134\n",
            "Epoch 7 Batch 150 Loss 3.2641 Accuracy 0.2126\n",
            "Epoch 7 Batch 200 Loss 3.2715 Accuracy 0.2126\n",
            "Epoch 7 Batch 250 Loss 3.2687 Accuracy 0.2134\n",
            "Epoch 7 Batch 300 Loss 3.2658 Accuracy 0.2137\n",
            "Epoch 7 Batch 350 Loss 3.2564 Accuracy 0.2145\n",
            "Epoch 7 Batch 400 Loss 3.2482 Accuracy 0.2154\n",
            "Epoch 7 Batch 450 Loss 3.2454 Accuracy 0.2157\n",
            "Epoch 7 Batch 500 Loss 3.2374 Accuracy 0.2165\n",
            "Epoch 7 Batch 550 Loss 3.2300 Accuracy 0.2170\n",
            "Epoch 7 Batch 600 Loss 3.2247 Accuracy 0.2173\n",
            "Epoch 7 Batch 650 Loss 3.2208 Accuracy 0.2177\n",
            "Epoch 7 Batch 700 Loss 3.2137 Accuracy 0.2180\n",
            "Epoch 7 Loss 3.2137 Accuracy 0.2180\n",
            "Time taken for 1 epoch: 466.27763199806213 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.6716 Accuracy 0.2415\n",
            "Epoch 8 Batch 50 Loss 2.9284 Accuracy 0.2301\n",
            "Epoch 8 Batch 100 Loss 2.9324 Accuracy 0.2299\n",
            "Epoch 8 Batch 150 Loss 2.9328 Accuracy 0.2299\n",
            "Epoch 8 Batch 200 Loss 2.9349 Accuracy 0.2305\n",
            "Epoch 8 Batch 250 Loss 2.9346 Accuracy 0.2309\n",
            "Epoch 8 Batch 300 Loss 2.9344 Accuracy 0.2322\n",
            "Epoch 8 Batch 350 Loss 2.9331 Accuracy 0.2321\n",
            "Epoch 8 Batch 400 Loss 2.9296 Accuracy 0.2321\n",
            "Epoch 8 Batch 450 Loss 2.9297 Accuracy 0.2319\n",
            "Epoch 8 Batch 500 Loss 2.9262 Accuracy 0.2325\n",
            "Epoch 8 Batch 550 Loss 2.9255 Accuracy 0.2326\n",
            "Epoch 8 Batch 600 Loss 2.9215 Accuracy 0.2329\n",
            "Epoch 8 Batch 650 Loss 2.9195 Accuracy 0.2331\n",
            "Epoch 8 Batch 700 Loss 2.9181 Accuracy 0.2333\n",
            "Epoch 8 Loss 2.9177 Accuracy 0.2333\n",
            "Time taken for 1 epoch: 468.93788480758667 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.7505 Accuracy 0.2328\n",
            "Epoch 9 Batch 50 Loss 2.6657 Accuracy 0.2461\n",
            "Epoch 9 Batch 100 Loss 2.6788 Accuracy 0.2435\n",
            "Epoch 9 Batch 150 Loss 2.6875 Accuracy 0.2433\n",
            "Epoch 9 Batch 200 Loss 2.6865 Accuracy 0.2447\n",
            "Epoch 9 Batch 250 Loss 2.6909 Accuracy 0.2455\n",
            "Epoch 9 Batch 300 Loss 2.6886 Accuracy 0.2456\n",
            "Epoch 9 Batch 350 Loss 2.6896 Accuracy 0.2451\n",
            "Epoch 9 Batch 400 Loss 2.6870 Accuracy 0.2454\n",
            "Epoch 9 Batch 450 Loss 2.6878 Accuracy 0.2453\n",
            "Epoch 9 Batch 500 Loss 2.6863 Accuracy 0.2452\n",
            "Epoch 9 Batch 550 Loss 2.6873 Accuracy 0.2452\n",
            "Epoch 9 Batch 600 Loss 2.6890 Accuracy 0.2454\n",
            "Epoch 9 Batch 650 Loss 2.6870 Accuracy 0.2457\n",
            "Epoch 9 Batch 700 Loss 2.6872 Accuracy 0.2459\n",
            "Epoch 9 Loss 2.6871 Accuracy 0.2460\n",
            "Time taken for 1 epoch: 474.35271167755127 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.2159 Accuracy 0.2730\n",
            "Epoch 10 Batch 50 Loss 2.4628 Accuracy 0.2577\n",
            "Epoch 10 Batch 100 Loss 2.4865 Accuracy 0.2551\n",
            "Epoch 10 Batch 150 Loss 2.4972 Accuracy 0.2537\n",
            "Epoch 10 Batch 200 Loss 2.5012 Accuracy 0.2552\n",
            "Epoch 10 Batch 250 Loss 2.5046 Accuracy 0.2552\n",
            "Epoch 10 Batch 300 Loss 2.5086 Accuracy 0.2557\n",
            "Epoch 10 Batch 350 Loss 2.5085 Accuracy 0.2555\n",
            "Epoch 10 Batch 400 Loss 2.5099 Accuracy 0.2559\n",
            "Epoch 10 Batch 450 Loss 2.5077 Accuracy 0.2558\n",
            "Epoch 10 Batch 500 Loss 2.5089 Accuracy 0.2560\n",
            "Epoch 10 Batch 550 Loss 2.5087 Accuracy 0.2563\n",
            "Epoch 10 Batch 600 Loss 2.5103 Accuracy 0.2562\n",
            "Epoch 10 Batch 650 Loss 2.5102 Accuracy 0.2563\n",
            "Epoch 10 Batch 700 Loss 2.5110 Accuracy 0.2563\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.5112 Accuracy 0.2563\n",
            "Time taken for 1 epoch: 475.74327778816223 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.3314 Accuracy 0.2758\n",
            "Epoch 11 Batch 50 Loss 2.3171 Accuracy 0.2663\n",
            "Epoch 11 Batch 100 Loss 2.3388 Accuracy 0.2682\n",
            "Epoch 11 Batch 150 Loss 2.3433 Accuracy 0.2672\n",
            "Epoch 11 Batch 200 Loss 2.3435 Accuracy 0.2669\n",
            "Epoch 11 Batch 250 Loss 2.3483 Accuracy 0.2655\n",
            "Epoch 11 Batch 300 Loss 2.3519 Accuracy 0.2651\n",
            "Epoch 11 Batch 350 Loss 2.3531 Accuracy 0.2647\n",
            "Epoch 11 Batch 400 Loss 2.3565 Accuracy 0.2648\n",
            "Epoch 11 Batch 450 Loss 2.3564 Accuracy 0.2642\n",
            "Epoch 11 Batch 500 Loss 2.3599 Accuracy 0.2641\n",
            "Epoch 11 Batch 550 Loss 2.3630 Accuracy 0.2646\n",
            "Epoch 11 Batch 600 Loss 2.3675 Accuracy 0.2649\n",
            "Epoch 11 Batch 650 Loss 2.3696 Accuracy 0.2653\n",
            "Epoch 11 Batch 700 Loss 2.3722 Accuracy 0.2650\n",
            "Epoch 11 Loss 2.3725 Accuracy 0.2650\n",
            "Time taken for 1 epoch: 468.4940447807312 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.1811 Accuracy 0.2611\n",
            "Epoch 12 Batch 50 Loss 2.2063 Accuracy 0.2734\n",
            "Epoch 12 Batch 100 Loss 2.2113 Accuracy 0.2734\n",
            "Epoch 12 Batch 150 Loss 2.2251 Accuracy 0.2724\n",
            "Epoch 12 Batch 200 Loss 2.2272 Accuracy 0.2721\n",
            "Epoch 12 Batch 250 Loss 2.2326 Accuracy 0.2719\n",
            "Epoch 12 Batch 300 Loss 2.2389 Accuracy 0.2725\n",
            "Epoch 12 Batch 350 Loss 2.2416 Accuracy 0.2719\n",
            "Epoch 12 Batch 400 Loss 2.2406 Accuracy 0.2721\n",
            "Epoch 12 Batch 450 Loss 2.2408 Accuracy 0.2721\n",
            "Epoch 12 Batch 500 Loss 2.2464 Accuracy 0.2722\n",
            "Epoch 12 Batch 550 Loss 2.2516 Accuracy 0.2718\n",
            "Epoch 12 Batch 600 Loss 2.2530 Accuracy 0.2715\n",
            "Epoch 12 Batch 650 Loss 2.2562 Accuracy 0.2713\n",
            "Epoch 12 Batch 700 Loss 2.2604 Accuracy 0.2714\n",
            "Epoch 12 Loss 2.2600 Accuracy 0.2714\n",
            "Time taken for 1 epoch: 468.12859416007996 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.0942 Accuracy 0.2620\n",
            "Epoch 13 Batch 50 Loss 2.1308 Accuracy 0.2744\n",
            "Epoch 13 Batch 100 Loss 2.1321 Accuracy 0.2745\n",
            "Epoch 13 Batch 150 Loss 2.1351 Accuracy 0.2732\n",
            "Epoch 13 Batch 200 Loss 2.1399 Accuracy 0.2739\n",
            "Epoch 13 Batch 250 Loss 2.1468 Accuracy 0.2755\n",
            "Epoch 13 Batch 300 Loss 2.1508 Accuracy 0.2758\n",
            "Epoch 13 Batch 350 Loss 2.1541 Accuracy 0.2760\n",
            "Epoch 13 Batch 400 Loss 2.1583 Accuracy 0.2768\n",
            "Epoch 13 Batch 450 Loss 2.1590 Accuracy 0.2764\n",
            "Epoch 13 Batch 500 Loss 2.1616 Accuracy 0.2763\n",
            "Epoch 13 Batch 550 Loss 2.1625 Accuracy 0.2763\n",
            "Epoch 13 Batch 600 Loss 2.1669 Accuracy 0.2764\n",
            "Epoch 13 Batch 650 Loss 2.1690 Accuracy 0.2763\n",
            "Epoch 13 Batch 700 Loss 2.1745 Accuracy 0.2762\n",
            "Epoch 13 Loss 2.1754 Accuracy 0.2762\n",
            "Time taken for 1 epoch: 467.0582139492035 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.1790 Accuracy 0.2932\n",
            "Epoch 14 Batch 50 Loss 2.0152 Accuracy 0.2856\n",
            "Epoch 14 Batch 100 Loss 2.0315 Accuracy 0.2860\n",
            "Epoch 14 Batch 150 Loss 2.0471 Accuracy 0.2837\n",
            "Epoch 14 Batch 200 Loss 2.0568 Accuracy 0.2832\n",
            "Epoch 14 Batch 250 Loss 2.0609 Accuracy 0.2824\n",
            "Epoch 14 Batch 300 Loss 2.0632 Accuracy 0.2827\n",
            "Epoch 14 Batch 350 Loss 2.0679 Accuracy 0.2825\n",
            "Epoch 14 Batch 400 Loss 2.0742 Accuracy 0.2823\n",
            "Epoch 14 Batch 450 Loss 2.0817 Accuracy 0.2821\n",
            "Epoch 14 Batch 500 Loss 2.0847 Accuracy 0.2819\n",
            "Epoch 14 Batch 550 Loss 2.0882 Accuracy 0.2817\n",
            "Epoch 14 Batch 600 Loss 2.0910 Accuracy 0.2814\n",
            "Epoch 14 Batch 650 Loss 2.0954 Accuracy 0.2812\n",
            "Epoch 14 Batch 700 Loss 2.0987 Accuracy 0.2814\n",
            "Epoch 14 Loss 2.0993 Accuracy 0.2814\n",
            "Time taken for 1 epoch: 466.8417682647705 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9143 Accuracy 0.2940\n",
            "Epoch 15 Batch 50 Loss 1.9593 Accuracy 0.2941\n",
            "Epoch 15 Batch 100 Loss 1.9577 Accuracy 0.2907\n",
            "Epoch 15 Batch 150 Loss 1.9688 Accuracy 0.2894\n",
            "Epoch 15 Batch 200 Loss 1.9899 Accuracy 0.2873\n",
            "Epoch 15 Batch 250 Loss 1.9981 Accuracy 0.2869\n",
            "Epoch 15 Batch 300 Loss 2.0060 Accuracy 0.2867\n",
            "Epoch 15 Batch 350 Loss 2.0131 Accuracy 0.2862\n",
            "Epoch 15 Batch 400 Loss 2.0163 Accuracy 0.2863\n",
            "Epoch 15 Batch 450 Loss 2.0173 Accuracy 0.2861\n",
            "Epoch 15 Batch 500 Loss 2.0204 Accuracy 0.2857\n",
            "Epoch 15 Batch 550 Loss 2.0270 Accuracy 0.2854\n",
            "Epoch 15 Batch 600 Loss 2.0313 Accuracy 0.2850\n",
            "Epoch 15 Batch 650 Loss 2.0373 Accuracy 0.2849\n",
            "Epoch 15 Batch 700 Loss 2.0388 Accuracy 0.2848\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.0391 Accuracy 0.2848\n",
            "Time taken for 1 epoch: 468.52838253974915 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.9170 Accuracy 0.2880\n",
            "Epoch 16 Batch 50 Loss 1.9222 Accuracy 0.2897\n",
            "Epoch 16 Batch 100 Loss 1.9350 Accuracy 0.2895\n",
            "Epoch 16 Batch 150 Loss 1.9415 Accuracy 0.2899\n",
            "Epoch 16 Batch 200 Loss 1.9449 Accuracy 0.2887\n",
            "Epoch 16 Batch 250 Loss 1.9523 Accuracy 0.2889\n",
            "Epoch 16 Batch 300 Loss 1.9556 Accuracy 0.2886\n",
            "Epoch 16 Batch 350 Loss 1.9586 Accuracy 0.2882\n",
            "Epoch 16 Batch 400 Loss 1.9609 Accuracy 0.2879\n",
            "Epoch 16 Batch 450 Loss 1.9647 Accuracy 0.2880\n",
            "Epoch 16 Batch 500 Loss 1.9693 Accuracy 0.2879\n",
            "Epoch 16 Batch 550 Loss 1.9753 Accuracy 0.2878\n",
            "Epoch 16 Batch 600 Loss 1.9787 Accuracy 0.2878\n",
            "Epoch 16 Batch 650 Loss 1.9835 Accuracy 0.2878\n",
            "Epoch 16 Batch 700 Loss 1.9875 Accuracy 0.2874\n",
            "Epoch 16 Loss 1.9880 Accuracy 0.2874\n",
            "Time taken for 1 epoch: 465.07887291908264 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.0011 Accuracy 0.2672\n",
            "Epoch 17 Batch 50 Loss 1.8427 Accuracy 0.2895\n",
            "Epoch 17 Batch 100 Loss 1.8532 Accuracy 0.2911\n",
            "Epoch 17 Batch 150 Loss 1.8703 Accuracy 0.2902\n",
            "Epoch 17 Batch 200 Loss 1.8776 Accuracy 0.2906\n",
            "Epoch 17 Batch 250 Loss 1.8883 Accuracy 0.2896\n",
            "Epoch 17 Batch 300 Loss 1.8983 Accuracy 0.2899\n",
            "Epoch 17 Batch 350 Loss 1.9090 Accuracy 0.2896\n",
            "Epoch 17 Batch 400 Loss 1.9158 Accuracy 0.2903\n",
            "Epoch 17 Batch 450 Loss 1.9177 Accuracy 0.2903\n",
            "Epoch 17 Batch 500 Loss 1.9219 Accuracy 0.2904\n",
            "Epoch 17 Batch 550 Loss 1.9279 Accuracy 0.2903\n",
            "Epoch 17 Batch 600 Loss 1.9332 Accuracy 0.2903\n",
            "Epoch 17 Batch 650 Loss 1.9366 Accuracy 0.2900\n",
            "Epoch 17 Batch 700 Loss 1.9414 Accuracy 0.2900\n",
            "Epoch 17 Loss 1.9411 Accuracy 0.2900\n",
            "Time taken for 1 epoch: 463.95084738731384 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.7874 Accuracy 0.2956\n",
            "Epoch 18 Batch 50 Loss 1.8174 Accuracy 0.2980\n",
            "Epoch 18 Batch 100 Loss 1.8284 Accuracy 0.2975\n",
            "Epoch 18 Batch 150 Loss 1.8423 Accuracy 0.2977\n",
            "Epoch 18 Batch 200 Loss 1.8564 Accuracy 0.2961\n",
            "Epoch 18 Batch 250 Loss 1.8671 Accuracy 0.2952\n",
            "Epoch 18 Batch 300 Loss 1.8717 Accuracy 0.2941\n",
            "Epoch 18 Batch 350 Loss 1.8759 Accuracy 0.2937\n",
            "Epoch 18 Batch 400 Loss 1.8806 Accuracy 0.2938\n",
            "Epoch 18 Batch 450 Loss 1.8829 Accuracy 0.2937\n",
            "Epoch 18 Batch 500 Loss 1.8855 Accuracy 0.2934\n",
            "Epoch 18 Batch 550 Loss 1.8896 Accuracy 0.2931\n",
            "Epoch 18 Batch 600 Loss 1.8932 Accuracy 0.2927\n",
            "Epoch 18 Batch 650 Loss 1.8971 Accuracy 0.2923\n",
            "Epoch 18 Batch 700 Loss 1.9011 Accuracy 0.2921\n",
            "Epoch 18 Loss 1.9018 Accuracy 0.2920\n",
            "Time taken for 1 epoch: 465.7484700679779 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.9737 Accuracy 0.2899\n",
            "Epoch 19 Batch 50 Loss 1.8228 Accuracy 0.2992\n",
            "Epoch 19 Batch 100 Loss 1.8292 Accuracy 0.2978\n",
            "Epoch 19 Batch 150 Loss 1.8218 Accuracy 0.2977\n",
            "Epoch 19 Batch 200 Loss 1.8233 Accuracy 0.2960\n",
            "Epoch 19 Batch 250 Loss 1.8245 Accuracy 0.2971\n",
            "Epoch 19 Batch 300 Loss 1.8319 Accuracy 0.2966\n",
            "Epoch 19 Batch 350 Loss 1.8342 Accuracy 0.2963\n",
            "Epoch 19 Batch 400 Loss 1.8398 Accuracy 0.2958\n",
            "Epoch 19 Batch 450 Loss 1.8434 Accuracy 0.2957\n",
            "Epoch 19 Batch 500 Loss 1.8477 Accuracy 0.2953\n",
            "Epoch 19 Batch 550 Loss 1.8517 Accuracy 0.2952\n",
            "Epoch 19 Batch 600 Loss 1.8563 Accuracy 0.2950\n",
            "Epoch 19 Batch 650 Loss 1.8621 Accuracy 0.2946\n",
            "Epoch 19 Batch 700 Loss 1.8669 Accuracy 0.2944\n",
            "Epoch 19 Loss 1.8672 Accuracy 0.2943\n",
            "Time taken for 1 epoch: 468.64103865623474 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.7225 Accuracy 0.2837\n",
            "Epoch 20 Batch 50 Loss 1.7755 Accuracy 0.2997\n",
            "Epoch 20 Batch 100 Loss 1.7714 Accuracy 0.2997\n",
            "Epoch 20 Batch 150 Loss 1.7801 Accuracy 0.2988\n",
            "Epoch 20 Batch 200 Loss 1.7814 Accuracy 0.2987\n",
            "Epoch 20 Batch 250 Loss 1.7867 Accuracy 0.2979\n",
            "Epoch 20 Batch 300 Loss 1.7896 Accuracy 0.2979\n",
            "Epoch 20 Batch 350 Loss 1.7943 Accuracy 0.2977\n",
            "Epoch 20 Batch 400 Loss 1.8033 Accuracy 0.2976\n",
            "Epoch 20 Batch 450 Loss 1.8086 Accuracy 0.2970\n",
            "Epoch 20 Batch 500 Loss 1.8125 Accuracy 0.2971\n",
            "Epoch 20 Batch 550 Loss 1.8177 Accuracy 0.2971\n",
            "Epoch 20 Batch 600 Loss 1.8244 Accuracy 0.2963\n",
            "Epoch 20 Batch 650 Loss 1.8308 Accuracy 0.2960\n",
            "Epoch 20 Batch 700 Loss 1.8356 Accuracy 0.2962\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.8357 Accuracy 0.2961\n",
            "Time taken for 1 epoch: 467.4784379005432 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9LJp4Nvftts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b315aa04-715e-4892-e7fb-a6a308da1476"
      },
      "source": [
        "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "  print(inp.shape, tar.shape)\n",
        "  if batch == 1:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 40) (64, 39)\n",
            "(64, 38) (64, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTH8yWLgBBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_pt.vocab_size]\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_en.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_bkIZU1gBwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_pt.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                        if i < tokenizer_en.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMSvS2_FgGj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  \n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
        "                                            if i < tokenizer_en.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qGwQcqkgLna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "401c6353-d327-48e5-ee89-99c7d8fb167c"
      },
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\n",
        "print (\"Real translation: this is a problem we have to solve .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: este é um problema que temos que resolver.\n",
            "Predicted translation: this is a problem that we have to solve the united states .\n",
            "Real translation: this is a problem we have to solve .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}