{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feed-forward.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakib103/pancancer_classification/blob/master/feed_forward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACmEisyfJncC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check GPU resources\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WIKNAmgJYfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1TdNixHQY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell to mount your Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1o0LFMSHVcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data and label\n",
        "\n",
        "data = pd.read_pickle(\"/content/drive/My Drive/main/data_df.pkl\")\n",
        "label = pd.read_pickle(\"/content/drive/My Drive/main/label_df.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64tTG2fHvuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check data and label integrity\n",
        "\n",
        "print(data)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygacYkrHiWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create data variable from loaded data\n",
        "\n",
        "data_points = 11069\n",
        "x = data.iloc[:data_points].values\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGRwFOrjH5KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create label variable from loaded label\n",
        "\n",
        "y = label.iloc[:data_points].values\n",
        "\n",
        "\n",
        "classes = ['GBM', 'OV', 'LUAD', 'LUSC', 'PRAD', 'UCEC', 'BLCA', 'TGCT', 'ESCA', 'PAAD', 'KIRP', 'LIHC', 'CESC', 'SARC', 'BRCA', 'THYM', 'MESO', 'COAD', 'STAD', 'SKCM', 'CHOL', 'KIRC', 'THCA', 'HNSC', 'LAML', 'READ', 'LGG', 'DLBC', 'KICH', 'UCS', 'ACC', 'PCPG', 'UVM']\n",
        "for id_y, lab_y in enumerate(y):\n",
        "  for id_class, lab_class in enumerate(classes):\n",
        "    if lab_y[0] == lab_class:\n",
        "      y[id_y] = id_class\n",
        "      break\n",
        "\n",
        "y = np.reshape(y, -1, 2)\n",
        "\n",
        "y = to_categorical(y)\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_J4IScSH-uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing - convert to log2 \n",
        "\n",
        "x = np.log2(x + 1)\n",
        "print (x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjWappCAIHKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing - reduce feature space based on variance threshold\n",
        "### need checking\n",
        "\n",
        "variance = np.var(x, axis=0)\n",
        "print(variance.shape)\n",
        "ommit_index = np.where(variance < 1.19)\n",
        "print (ommit_index)\n",
        "x = np.delete(x, ommit_index, axis = 1)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDbHjri3IPd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle the data and label if necessary\n",
        "## same patient check\n",
        "## concatenate x, y then shuffle\n",
        "## check if diff generate each time\n",
        "\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(x)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNkWd2gGIsPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom losss function - wighted categorical-crossentropy\n",
        "## need testing\n",
        "\"\"\"\n",
        "A weighted version of categorical_crossentropy for keras (2.0.6). This lets you apply a weight to unbalanced classes.\n",
        "@url: https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
        "@author: wassname\n",
        "\"\"\"\n",
        "from keras import backend as K\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    weights = K.variable(weights)\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NlmJh_A2U2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model definition\n",
        "\n",
        "regularizer = regularizers.l1_l2(l1=0.000, l2=0.000)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 512, input_shape = (x.shape[1],), activation = 'relu', kernel_regularizer = regularizer))\n",
        "model.add(Dropout(rate = 0.1, noise_shape=None, seed=None))\n",
        "\n",
        "model.add(Dense(units = 512, activation = 'relu', kernel_regularizer = regularizer))\n",
        "model.add(Dropout(rate = 0.1, noise_shape=None, seed=None))\n",
        "\n",
        "# model.add(Dense(units = 512, activation = 'relu', kernel_regularizer = regularizer))\n",
        "# model.add(Dropout(rate = 0.1, noise_shape=None, seed=None))\n",
        "\n",
        "model.add(Dense(units = y.shape[1], activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-tajRK065Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define learning process\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=True)\n",
        "adam  = optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "weights = [174, 309, 576, 554, 550, 567, 427, 156, 196, 183, 323, 424, 310, 265, 1218, 122, 87, 495, 450, 474, 45, 606, 572, 566, 173, 171, 534, 48, 91, 57, 79, 187, 80]\n",
        "weights = [ weight/11069 for weight in weights]\n",
        "\n",
        "model.compile(loss = weighted_categorical_crossentropy(weights), optimizer = adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHP3rJce1Yk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "\n",
        "history = model.fit(x, y, validation_split = 0.1, batch_size=256, epochs=300, verbose=1, shuffle=True)\n",
        "with open('/content/drive/My Drive/main/2H002', 'wb') as file_pi:\n",
        "  pickle.dump(history.history, file_pi)\n",
        "  print(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-BjsTaVCE-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K-fold validation\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train, test in kfold.split(x, y):\n",
        "  \n",
        "  regularizer = regularizers.l1_l2(l1=0.0000001, l2=0.00001)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units = 4096, input_shape = (x.shape[1],), activation = 'relu', kernel_regularizer = regularizer))\n",
        "  model.add(Dropout(rate = 0.3, noise_shape=None, seed=None))\n",
        "  # model.add(Dense(units = 1024, activation = 'relu', kernel_regularizer = regularizer))\n",
        "  model.add(Dense(units = 2048, activation = 'relu', kernel_regularizer = regularizer))\n",
        "  model.add(Dropout(rate = 0.3, noise_shape=None, seed=None))\n",
        "  model.add(Dense(units = 512, activation = 'relu', kernel_regularizer = regularizer))\n",
        "  model.add(Dropout(rate = 0.3, noise_shape=None, seed=None))\n",
        "  # model.add(Dense(units = 4096, activation = 'relu'))\n",
        "  model.add(Dense(units = y.shape[1], activation = 'softmax'))\n",
        "\n",
        "  sgd = optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=True)\n",
        "  adam  = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
        "  weights = [174, 309, 576, 554, 550, 567, 427, 156, 196, 183, 323, 424, 310, 265, 1218, 122, 87, 495, 450, 474, 45, 606, 572, 566, 173, 171, 534, 48, 91, 57, 79, 187, 80]\n",
        "  weights = [ weight/11069 for weight in weights]\n",
        "  model.compile(loss = weighted_categorical_crossentropy(weights), optimizer = adam, metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x[train], y[train], validation_split = 0.0, batch_size=256, epochs=100, verbose=1, shuffle=True)\n",
        "  with open('/content/drive/My Drive/main/2H002', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "#     print(history)\n",
        "    scores = model.evaluate(x[test], y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}